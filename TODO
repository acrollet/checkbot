*  Doing a HEAD on an ISMAP link causes a 500 server error. Maybe a 
   GET will be better for this.

*  Add a mechanism for entering authentication data in order to check
   such areas as well. Possibly allow multiple pairs. Rather not enter
   the passwords on the commandline.

*  Obey Robot rules. My current idea is to obey robot rules by default
   on all 'external' request, and ignore them on 'local'
   requests. Both of these should be changeable.

*  Make it possible to convert http:// references to file://
   references, so that a local server can be checked without going
   through the WWW server.

*  If the first link causes an error like 404/File not found, then we
   might want to treat this specially to avoid confusion.

*  Make use of some sort of state, so that subsequent runs can choose
   to just check the broken links again.

*  Retry time-out problem links after checking all other links to
   better deal with transient problems.

*  Add problem links such as the 500's series together.
